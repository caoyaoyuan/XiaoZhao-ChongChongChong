* 决策树
#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成9）/截屏2020-06-10 下午5.26.23.png @ 2020-06-10 17:26:27
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_17-26-27_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%885.26.23.png]]

** 1.1 定义
**** 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点由两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。
**** 决策树学习通常包含3个步骤：特征选择、决策树的生成和决策树的剪枝。

** 1.2 决策树的特征选择
*** 1.2.1 信息增益
*** 熵（entropy）
#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成10）/截屏2020-06-10 下午7.19.19.png @ 2020-06-10 19:19:22
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-19-22_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.19.19.png]]

**** 熵越大，随机变量的不确定性就越大。
*** 条件熵 
**** H(Y|X) 表示在已知随机变量X的条件下随机变量Y的不确定性
#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成11）/截屏2020-06-10 下午7.21.33.png @ 2020-06-10 19:21:35
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-21-35_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.21.33.png]]

*** 信息增益
#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成12）/截屏2020-06-10 下午7.24.21.png @ 2020-06-10 19:24:23
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-24-23_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.24.21.png]]

**** 决策树中某个特征 A 对应的信息增益，表示由于特征 A 而使得对数据集 D 的分类的不确定减少的程度。不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力（即优先选择信息增益大的特征）。
*** 信息增益算法示例

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成13）/截屏2020-06-10 下午7.35.31.png @ 2020-06-10 19:35:36
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-35-36_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.35.31.png]]

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成14）/截屏2020-06-10 下午7.36.33.png @ 2020-06-10 19:36:37
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-36-37_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.36.33.png]]

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成15）/截屏2020-06-10 下午7.37.16.png @ 2020-06-10 19:37:22
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-37-22_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.37.16.png]]

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成16）/截屏2020-06-10 下午7.38.01.png @ 2020-06-10 19:38:05
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-38-05_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.38.01.png]]

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成17）/截屏2020-06-10 下午7.38.37.png @ 2020-06-10 19:38:41
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-38-41_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.38.37.png]]

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成18）/截屏2020-06-10 下午7.39.04.png @ 2020-06-10 19:39:07
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-39-07_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.39.04.png]]


*** 1.2.2 信息增益率
**** 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比（information gain ratio）可以对这一类问题进行校正。信息增益比是特征选择的另一准则。

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成19）/截屏2020-06-10 下午7.40.47.png @ 2020-06-10 19:40:52
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-40-52_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.40.47.png]]

** 1.3 决策树的生成 
**** 1.信息增益 -> ID3 算法
**** 2.信息增益率 -> C4.5 算法
**** 3.基尼指数 -> CART 算法
*** 1.3.1 ID3 算法

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成20）/截屏2020-06-10 下午7.58.59.png @ 2020-06-10 19:59:03
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_19-59-03_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%887.58.59.png]]

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成21）/截屏2020-06-10 下午8.05.56.png @ 2020-06-10 20:06:00
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_20-06-00_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%888.05.56.png]]

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成22）/截屏2020-06-10 下午8.06.30.png @ 2020-06-10 20:06:34
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_20-06-34_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%888.06.30.png]]

**** ID3 算法只有树的生成，所以该算法生成的树容易产生过拟合。
*** 1.3.2 C4.5 算法
**** 与ID3算法类似，C4.5在生成树的过程中，用信息增益来选择特征。
**** ID3、C4.5算法生成的决策树只能做分类，下文的CART树既可以做分类也可以做回归
*** 1.3.3 CART 算法
**** CART（classification and regression tree）同样由特征选择、树的生成及剪枝组成，可以做分类和回归。
**** CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支为“是”，右分支为“否”；依次递归地二分每个特征。
**** CART 对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则。进行特征选择，生成二叉树。
**** 1.3.3.1 回归树

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成23）/截屏2020-06-10 下午8.41.18.png @ 2020-06-10 20:41:22
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_20-41-22_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%888.41.18.png]]

**** 1.3.3.2 分类树
**** 是一棵二叉树。
#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成24）/截屏2020-06-10 下午8.51.19.png @ 2020-06-10 20:51:23
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_20-51-23_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%888.51.19.png]]

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成25）/截屏2020-06-10 下午8.51.44.png @ 2020-06-10 20:51:49
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_20-51-49_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%888.51.44.png]]

**** 基尼指数越小，样本纯度越高。
*** 1.3.4 决策树何时划分结束？
**** 1.叶子结点中全部都是一种类型（或小于某个阈值）
**** 2.没有特征可以划分
*** 1.3.5 小结
**** 决策树的生成，通常使用信息增益最大、信息增益率最大或基尼指数最小作为特征选择的准则。
** 1.4 决策树的剪枝
*** 1.4.1 后剪枝
**** 决策树生成只考虑通过提高信息增益/信息增益率对训练数据进行更好的拟合；决策树剪枝除了考虑经验风险损失外，还考虑减小模型复杂度！

#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成26）/截屏2020-06-10 下午8.57.59.png @ 2020-06-10 20:58:02
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_20-58-02_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%888.57.59.png]]
**** |T|可以表示为树T中叶结点的个数（可以衡量树的复杂度）
**** 参数 a 权衡训练数据的拟合程度与模型的复杂度。
#+DOWNLOADED: file:/var/folders/wk/9k90t6fs7kx91_cn9v90hx_00000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成27）/截屏2020-06-10 下午8.58.49.png @ 2020-06-10 20:58:53
[[file:Screen-Pictures/%E5%86%B3%E7%AD%96%E6%A0%91/2020-06-10_20-58-53_%E6%88%AA%E5%B1%8F2020-06-10%20%E4%B8%8B%E5%8D%888.58.49.png]]
**** 决策树生成学习局部的模型；决策树剪枝学习整体的模型。
*** 1.4.2 预剪枝
**** 预剪枝是在树的生成过程中进行剪枝。一般基于“贪心”策略，可能造成局部最优。
**** 后剪枝的运算量较大，但是比较精准。

* 随机森林
** 1.简单概括
**** 随机森林是一种集成算法，属于Bagging类型；通过组合多个弱分类器，最终结果通过投票或取均值，使得整体模型的结果具有较高的精确度和泛化性能。
**** “随机”和“森林”，一个使它具有抗过拟合能力，一个使它更加精准。
** 2.Bagging简介
**** Bagging也叫自举汇聚法（bootstrap aggregating），是一种在原始数据集上通过有放回抽样重新选出k个新数据集来训练分类器的集成技术。
**** 它使用训练出来的分类器的集合来对新样本进行分类，然后用多数投票或者对输出求均值的方法统计所有分类器的分类结果，结果最高的类别即为最终标签。此类算法可以显著降低方差。
** 3.随机森林
*** 3.1 弱分类器
**** RF使用了CART决策树作为弱学习器，即我们只是将使用CART决策树作为弱学习器的Bagging方法称为随机森林。
*** 3.2 随机性
**** 2个随机：森林中每棵树的训练样本是随机选取的；树的特征也是随机选取。
*** 3.3 特点
**** 由于随机性，RF对于降低模型的方差很有作用，故随机森林一般不需要额外做剪枝，即可以取得较好的泛化能力和抗过拟合能力（Low Variance）。
**** 相对而言，模型对于训练集的拟合程度就会差一些，相比于GBDT模型的偏差（bias）会大一些。
**** 另外，随机森林树的深度一般会比较深，以尽可能地降低bias；而GBDT树的深度会比较浅，通过降低模型复杂度来降低variance（面试考点）。

* Reference
*** 1.[[https://www.jianshu.com/p/a779f0686acc][随机森林原理介绍与适用情况（综述篇）]]
*** 2李航《统计学习方法》第二版，第五章 决策树.(p67-p88)
