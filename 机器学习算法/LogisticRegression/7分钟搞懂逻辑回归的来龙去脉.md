# 7分钟搞懂逻辑回归的来龙去脉

![7分钟搞懂逻辑回归的来龙去脉](https://pic3.zhimg.com/v2-bc7f88302e042231963590b8462bbb28_1440w.jpg?source=172ae18b)

> 作者：Giant
>
> 来源：公众号【NLP情报局】

逻辑回归（Logistic Regression）是一种统计机器学习方法，简单易用，却涉及很多知识点。正所谓麻雀虽小，五脏俱全。

大多数教程都是从定义和原理出发，来讲解逻辑回归，容易显得晦涩难懂。本文将结合实例和图示，帮助读者在**7**分钟内搞懂逻辑回归算法。

## **1、功能**

逻辑回归一般用于**二分类**任务，并能给出两个类的相应**概率**。

常见的应用包括垃圾邮件判别、银行判断是否给用户贷款等等。当然，二分类问题可以扩展到多分类问题。

做二分类任务，最简单的判别函数是**阶跃函数**，如下图红线所示。当时判断为正类（1），反之为负类（0）。

![img](https://pic4.zhimg.com/80/v2-ce5a79043f4f9b0ce220dff98bf934c3_720w.jpg)

但阶跃函数不连续，过于“死板”，不便于后续求导优化。因此用logistic function（上图黑线）代替，因为呈现“S”形，也称为 sigmoid function，对应公式：

![[公式]](https://www.zhihu.com/equation?tex=y%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D)

定义域为整个实数集合，值域为0～1，相当于概率值。

## **2、为何不叫逻辑分类？**

既然是做分类任务，为什么不叫它“逻辑分类”呢？

首先，“逻辑”指的是“logistic”（音译），“回归”来源于线性回归的 ，**使用线性回归去拟合逼近一个决策边界**，使得按照这个边界进行数据分类后的总损失最小。

**以概率0.5作为界线，将数据分为正例和反例**。当 ![[公式]](https://www.zhihu.com/equation?tex=z%3E0) ，对应正例（趋近于概率1）；当 ![[公式]](https://www.zhihu.com/equation?tex=z%3C0) ，对应负例（趋近于概率0）。

这是在**使用回归的思想去解决分类问题**，所以称为逻辑回归。等价于在线性回归外包裹了一层sigmoid函数，将离散值映射为0和1之间的概率，以0.5为界。

## **3、核心问题**

理解逻辑回归的一个核心问题是，如何求解**决策边界** ![[公式]](https://www.zhihu.com/equation?tex=z%3DXW) ?

对于二维输入样本点，z 等价于：

![[公式]](https://www.zhihu.com/equation?tex=z%3DXW%3Dw_%7B0%7D%2Bw_%7B1%7Dx_%7B1%7D%2Bw_%7B2%7Dx_%7B2%7D)

求最优决策边界，等价于求 ![[公式]](https://www.zhihu.com/equation?tex=w_%7B0%7D%2Cw_%7B1%7D%2Cw_%7B2%7D) 的值。当样本的真实标签 ![[公式]](https://www.zhihu.com/equation?tex=y%5E%7B%2A%7D) 是1和0时，我们分别定义一个损失函数：

![img](https://pic3.zhimg.com/80/v2-7e90b85e083d4743ac3c2d3e7fae59fa_720w.jpg)

以 ![[公式]](https://www.zhihu.com/equation?tex=y%5E%7B%2A%7D%3D1) 为例，当模型的预测值 ![[公式]](https://www.zhihu.com/equation?tex=h) 趋向1时，损失函数取值也应该越来越小；反之，当 ![[公式]](https://www.zhihu.com/equation?tex=h) 趋向0时，损失函数值越来越大，可以通过函数 ![[公式]](https://www.zhihu.com/equation?tex=cost%3D-log%28h%29) 体现。模型的训练目的是**尽可能减小损失**，所以会让输出值朝着1的方向学习。是否可以将两类的cost函数合并到一块，方便计算总损失呢？

通过一个“聪明”的**对数似然函数，**我们达到了目的：

![[公式]](https://www.zhihu.com/equation?tex=cost%3D-y%5E%7B%2A%7Dlog%28h%29-%281-y%5E%7B%2A%7D%29log%281-h%29)

其中： ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bmodel%7D%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D)

对于下图的样本点，绿线是**决策边界**。绿线上部 ![[公式]](https://www.zhihu.com/equation?tex=z%3E0) ，距离绿线越远 ![[公式]](https://www.zhihu.com/equation?tex=z) 越大，预测值 ![[公式]](https://www.zhihu.com/equation?tex=h) 越接近1。

![img](https://pic1.zhimg.com/80/v2-cf49cee05fbb826ac5f288117fb0842c_720w.jpg)

## **4、求解边界**

明确了损失函数后，我们来计算模型参数的最优值。首先需要计算cost对参数 ![[公式]](https://www.zhihu.com/equation?tex=w) 的导数，再借助**梯度下降**等算法微调参数值，不断逼近最优解。

假设我们有10个样本点，每个样本包含3个特征，则 ![[公式]](https://www.zhihu.com/equation?tex=X) 维度为[10, 3]， ![[公式]](https://www.zhihu.com/equation?tex=W) 维度为[3, 1]， ![[公式]](https://www.zhihu.com/equation?tex=Z) 和 ![[公式]](https://www.zhihu.com/equation?tex=H) 的维度为[10, 1]。

![[公式]](https://www.zhihu.com/equation?tex=Z%3DXW%2CH_%7Bmodel%7D%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-Z%7D%7D%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-XW%7D%7D)

损失函数：

![[公式]](https://www.zhihu.com/equation?tex=cost%3D-Y%5E%7B%2A%7Dlog%28H%29-%281-Y%5E%7B%2A%7D%29log%281-H%29)

cost的维度也是[10, 1]。cost和H相关，H和Z相关，Z和WX相关，存在关系映射：cost~H~Z~X。根据**链式求导法则**，整个计算过程如下：

![img](https://pic1.zhimg.com/80/v2-5637920f9d4f03adac4f2d8e09044e44_720w.jpg)

最终的结果是： ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7Bd_%7Bcost%7D%7D%7Bd_%7BW%7D%7D%3DX%5E%7BT%7D%28H-Y%5E%7B%2A%7D%29) ，维度是[3, 1]，即参数 ![[公式]](https://www.zhihu.com/equation?tex=w_%7B1%7D%2Cw_%7B2%7D%2Cw_%7B3%7D) 。

## **5、梯度下降法**

刚刚我们使用了梯度下降法迭代求解最优的 ，一共分为3步：

- 初始化 ![[公式]](https://www.zhihu.com/equation?tex=W)
- 更新 ![[公式]](https://www.zhihu.com/equation?tex=W) : ![[公式]](https://www.zhihu.com/equation?tex=W-%3D%5Calpha%2A%5Cfrac%7Bd_%7Bcost%7D%7D%7Bd_%7BW%7D%7D)
- 迭代到一定次数或阈值，结束

当cost函数是凸函数时，可以保证cost降到全局最小，否则可能只走到局部最小。

![img](https://pic2.zhimg.com/80/v2-36b1ecb62e14d68d1b6d19e197a2c7f9_720w.jpg)

在cost不断减小的过程中，将求得最优的分界线。

![img](https://pic3.zhimg.com/80/v2-c070b491402fc3519b06210fe86ea2c6_720w.jpg)

使用逻辑回归，我们可以使用python、C++等语言自己实现，或借助机器学习工具包Sklearn中的接口 **LogisticRegression** [2]。

现在，大家是不是理解了逻辑回归的思想呢？如有疑问，欢迎交流！（vx：cs-yechen）



## **深度学习资源下载**

**在公众号「NLP情报局」后台回复“三件套”，即可获取深度学习三件套：**

**《PyTorch深度学习》，《Hands-on Machine Learning》，《Python深度学习》**

![img](https://mmbiz.qpic.cn/mmbiz_png/V23nFVyYSrAyUicfJ9dDvyqXqjysKXkAlWt1ROwXHmFn0cqcicpY0JVDeibIJwNjPWiaYF3pOzPehyvvHxZa2mFoIg/640?wx_fmt=png)



## 参 考 文 献

[1] 文小刀机器学习｜逻辑回归：[https://www.bilibili.com/video/BV1As411j7zw](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1As411j7zw)

[2] [sklearn.linear_model.LogisticRegression](https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)

