* skip-gram
参考链接：[[https://zhuanlan.zhihu.com/p/27234078]]
** 模型定义
#+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成51）/截屏2020-06-02 下午3.41.38.png @ 2020-06-02 15:41:43
[[file:Screen-Pictures/skip-gram/2020-06-02_15-41-43_%E6%88%AA%E5%B1%8F2020-06-02%20%E4%B8%8B%E5%8D%883.41.38.png]]
建模过程中需要得到副产物词向量矩阵
** 输入和输出部分选择
#+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成45）/截屏2020-06-02 上午11.11.14.png @ 2020-06-02 11:11:17
[[file:Screen-Pictures/skip-gram/2020-06-02_11-11-17_%E6%88%AA%E5%B1%8F2020-06-02%20%E4%B8%8A%E5%8D%8811.11.14.png]]
设置窗口大小，以及滑动步长
** 模型结构
#+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成46）/截屏2020-06-02 上午11.30.40.png @ 2020-06-02 11:30:43
[[file:Screen-Pictures/skip-gram/2020-06-02_11-30-43_%E6%88%AA%E5%B1%8F2020-06-02%20%E4%B8%8A%E5%8D%8811.30.40.png]]
输入为one-hot向量，隐藏层无激活函数，输出为词表空间维度的概率（V，），经过softmax激活，一般会采用分层softmax加快速度。
** 隐藏层
#+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成47）/截屏2020-06-02 下午1.59.29.png @ 2020-06-02 13:59:33
[[file:Screen-Pictures/skip-gram/2020-06-02_13-59-33_%E6%88%AA%E5%B1%8F2020-06-02%20%E4%B8%8B%E5%8D%881.59.29.png]]
隐藏层本身即为需要得到的词向量矩阵，输出为输入词的词向量，（1，V）*（V，300）=（1，300），但实际上不是进行矩阵乘法，而是采用查找表的形式根据输入向量为1的维度作为索引行取得词向量，作为隐藏层的输出。
#+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成48）/截屏2020-06-02 下午2.04.03.png @ 2020-06-02 14:04:06
[[file:Screen-Pictures/skip-gram/2020-06-02_14-04-06_%E6%88%AA%E5%B1%8F2020-06-02%20%E4%B8%8B%E5%8D%882.04.03.png]]
** 输出层
输出层是一个(300, V)的矩阵，隐藏层的输出(1, 300)*(300, V)=(1,V)得到输出层在词表空间的输出概率
#+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成52）/截屏2020-06-02 下午3.46.40.png @ 2020-06-02 15:46:45
[[file:Screen-Pictures/skip-gram/2020-06-02_15-46-45_%E6%88%AA%E5%B1%8F2020-06-02%20%E4%B8%8B%E5%8D%883.46.40.png]]
直观上相当于将输入vector乘以输出vector，得到相似度分数，经过softmax得到预测概率，符合实际情况。loss的计算应该是用交叉墒计算，最大化输出词语所在词表维度的似然概率
** 训练过程
*** 存在问题
   + 模型权重巨大，梯度下降相当慢
   + 训练数据数量庞大，导致训练很繁重
   + 高频词例如‘the’对词语语义的贡献微乎其微，且高频词数目很大，带来了很多无关的训练样本
   + 词语组合具有特殊意义，例如'New York'
*** 解决方案
**** 高频词抽样
     对于训练原始文本中遇到的每一个词语，都有一定概率被删除，被删除的概率和单词频率成正比
#+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成53）/截屏2020-06-02 下午4.13.58.png @ 2020-06-02 16:14:02
[[file:Screen-Pictures/skip-gram/2020-06-02_16-14-02_%E6%88%AA%E5%B1%8F2020-06-02%20%E4%B8%8B%E5%8D%884.13.58.png]]
y-单词被保留的概率，x-单词出现频率
**** word pairs and 'phases'
     对特殊的单词组合不进行拆分
**** negative sampling
     + 参考链接：[[https://blog.csdn.net/qq_34467412/article/details/95861279]]
     + 从负标签中采样num_sample个负标签，和正标签计算softmax，减少计算量
       #+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成58）/截屏2020-06-02 下午4.53.40.png @ 2020-06-02 16:53:42
       [[file:Screen-Pictures/skip-gram/2020-06-02_16-53-42_%E6%88%AA%E5%B1%8F2020-06-02%20%E4%B8%8B%E5%8D%884.53.40.png]]
       #+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成59）/截屏2020-06-02 下午4.54.04.png @ 2020-06-02 16:54:07
       [[file:Screen-Pictures/skip-gram/2020-06-02_16-54-07_%E6%88%AA%E5%B1%8F2020-06-02%20%E4%B8%8B%E5%8D%884.54.04.png]]
       #+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成61）/截屏2020-06-02 下午4.55.41.png @ 2020-06-02 16:55:44
       [[file:Screen-Pictures/skip-gram/2020-06-02_16-55-44_%E6%88%AA%E5%B1%8F2020-06-02%20%E4%B8%8B%E5%8D%884.55.41.png]]
     + 一个单词被选作负样本的概率正比于出现频次
* cbow
参考链接：[[https://www.jianshu.com/p/d2f0759d053c]]
** 模型定义
#+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成63）/截屏2020-06-03 上午10.47.37.png @ 2020-06-03 10:47:40
[[file:Screen-Pictures/cbow/2020-06-03_10-47-40_%E6%88%AA%E5%B1%8F2020-06-03%20%E4%B8%8A%E5%8D%8810.47.37.png]]
输入上下文，输出目标词
** 模型结构
#+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成64）/截屏2020-06-03 上午10.49.00.png @ 2020-06-03 10:49:03
[[file:Screen-Pictures/cbow/2020-06-03_10-49-03_%E6%88%AA%E5%B1%8F2020-06-03%20%E4%B8%8A%E5%8D%8810.49.00.png]]
与skip-gram不同的是，输入的上下文通过隐藏层的词向量矩阵得到n个词向量后，相加取平均作为隐藏层的输出(1, 300)
** 问题探讨
   + 隐藏层和输出层的权重矩阵都是词向量，但是我们只用了隐藏层的词向量，实际上也可以2个词向量矩阵取平均作为目标词向量矩阵
* 层级softmax
  + 参考链接：
    + [[https://zhuanlan.zhihu.com/p/56139075]]
    + [[https://zhuanlan.zhihu.com/p/32965521]]
  + Logitic Regression：利用Sigmoid函数把任意值映射到（0,1）的区间上来实现分类问题（主要是二分类）,在这里的应用是判断在哈夫曼🌲中是走左子树还是右子🌲的概率
  + softmax: 多分类的Logistic Regression，相当于把很多个Logistic Regression组合在一起
  + 哈夫曼树
    #+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成69）/截屏2020-06-04 上午10.35.30.png @ 2020-06-04 10:35:32
    [[file:Screen-Pictures/%E5%B1%82%E7%BA%A7softmax/2020-06-04_10-35-32_%E6%88%AA%E5%B1%8F2020-06-04%20%E4%B8%8A%E5%8D%8810.35.30.png]]
  + 模型结构变化
    #+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成75）/截屏2020-06-05 下午4.56.42.png @ 2020-06-05 16:56:45
    [[file:Screen-Pictures/%E5%B1%82%E7%BA%A7softmax/2020-06-05_16-56-45_%E6%88%AA%E5%B1%8F2020-06-05%20%E4%B8%8B%E5%8D%884.56.42.png]]
    #+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成78）/截屏2020-06-05 下午5.11.46.png @ 2020-06-05 17:11:48
    [[file:Screen-Pictures/%E5%B1%82%E7%BA%A7softmax/2020-06-05_17-11-48_%E6%88%AA%E5%B1%8F2020-06-05%20%E4%B8%8B%E5%8D%885.11.46.png]]
    #+DOWNLOADED: file:/var/folders/73/53s3wczx1l32608prn_fdgrm0000gn/T/TemporaryItems/（screencaptureui正在存储文稿，已完成77）/截屏2020-06-05 下午5.11.08.png @ 2020-06-05 17:11:09
    [[file:Screen-Pictures/%E5%B1%82%E7%BA%A7softmax/2020-06-05_17-11-09_%E6%88%AA%E5%B1%8F2020-06-05%20%E4%B8%8B%E5%8D%885.11.08.png]]
  + 原理：相当于做了n次二分类逻辑回归，将原来的复杂度O(V)降低为O(log(V))，哈夫曼🌲的每一个节点都保存一个需要训练的参数，替代原来的（N，V)的词向量矩阵。得到某个目标词的概率的话只需要计算它的路径上的节点的概率之积。词表中词频越大的词越靠近根节点，被分类的概率也就越大，符合实际情况

